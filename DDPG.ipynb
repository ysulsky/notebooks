{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ysulsky/notebooks/blob/master/DDPG.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "kWq5mk34XFPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Following https://arxiv.org/abs/1509.02971\n",
        "\n",
        "---\n",
        "**Bellman Equation for Q-functions, with a deterministic policy**\n",
        "\n",
        "$Q(s_t, a_t | \\theta_Q) = y_t$\n",
        "\n",
        "where:\n",
        "\n",
        "$y_t = E_{r_t,s_{t+1}}\\big[r_t + \\gamma Q(s_{t+1}, a_{t+1} | \\theta'_Q)\\big]$ \\\\\n",
        "$r_t \\sim R(s_t, a_t)$ \\\\\n",
        "$s_{t + 1} \\sim T(s_t, a_t)$ \\\\\n",
        "$a_{t + 1} = \\pi(s_{t + 1} | \\theta'_\\pi)$\n",
        "\n",
        "---\n",
        "**Critic Objective**\n",
        "\n",
        "$L_Q(s_t, a_t, \\theta_Q) = (Q(s_t, a_t | \\theta_Q) - y_t)^2$\n",
        "\n",
        "---\n",
        "**Policy Objective**\n",
        "\n",
        "$L_\\pi(s_t, \\theta_Q, \\theta_\\pi) = -Q(s_t, \\pi(s_t | \\theta_\\pi) | \\theta_Q)$\n",
        "\n",
        "---\n",
        "**Target Update Rule**\n",
        "\n",
        "$\\theta'_Q \\leftarrow \\tau \\cdot \\theta_Q + (1 - \\tau) \\cdot \\theta'_Q$ \\\\\n",
        "$\\theta'_\\pi \\leftarrow \\tau \\cdot \\theta_\\pi + (1 - \\tau) \\cdot \\theta'_\\pi$ \\\\\n",
        "\n",
        "where $\\tau \\ll 1$"
      ]
    },
    {
      "metadata": {
        "id": "to7lSMTQt9MG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a1b4ad18-5aef-4161-8712-75b1410bddcf"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "  Downloading gym-0.10.5.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 648kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "  Downloading pyglet-1.3.1-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 996kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f8/88/f2/22e53080a462567706fad31295462941b3b06b16b51c3ab3e1\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.5 pyglet-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mc28dSJ1ygm3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oC7EDmr5TW0e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Replay Buffer\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size=10000):\n",
        "    self._arr = []\n",
        "    self._max_size = max_size\n",
        "    self._start = 0\n",
        "    \n",
        "  def record_transition(self, transition):\n",
        "    if len(self._arr) < self._max_size:\n",
        "      self._arr.append(transition)\n",
        "    else:\n",
        "      self._arr[self._start] = transition\n",
        "      self._start = (self._start + 1) % self._max_size\n",
        "      \n",
        "  def sample_batch(self, batch_size):\n",
        "    # np.random.choice doesn't work here because self._arr isn't 1-D\n",
        "    return [self._arr[i] \n",
        "            for i in np.random.randint(0, len(self._arr), size=batch_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5p54YuAVITS9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Wrapper around Gym and ReplayBuffer to operate in the TF graph.\n",
        "\n",
        "class Env(object):\n",
        "  def __init__(self, name, reward_decay=0.99):\n",
        "    self._env = gym.make(name)\n",
        "    self._replay = ReplayBuffer()\n",
        "    self._obs = self._env.reset()\n",
        "    self._state_dtype = self.observation_space.dtype\n",
        "    self._state_shape = self.observation_space.shape\n",
        "    self._total_rewards = 0.\n",
        "    self._num_episodes = 0\n",
        "    self._current_episode_total = 0\n",
        "    self._current_timestep = 0\n",
        "    self._reward_decay = reward_decay\n",
        "\n",
        "  \n",
        "  def mean_episodic_reward(self):\n",
        "    return ((self._total_rewards + self._current_episode_total) /\n",
        "            (self._num_episodes + 1.))\n",
        "\n",
        "  \n",
        "  @property\n",
        "  def reward_decay(self):\n",
        "    return self._reward_decay\n",
        "\n",
        "  \n",
        "  @property\n",
        "  def action_space(self):\n",
        "    return self._env.action_space\n",
        "\n",
        "  \n",
        "  @property\n",
        "  def observation_space(self):\n",
        "    return self._env.observation_space\n",
        "  \n",
        "\n",
        "  def _py_observe(self):\n",
        "    return np.asarray(self._obs, dtype=self.observation_space.dtype)\n",
        "    \n",
        "  \n",
        "  def observe(self):\n",
        "    state, = tf.py_func(self._py_observe, [], [self._state_dtype], \n",
        "                        name=\"observe\")\n",
        "    state.set_shape(self._state_shape)\n",
        "    return tf.expand_dims(state, 0)  # Add a batch dimension.\n",
        "\n",
        "  \n",
        "  def _py_step(self, action):\n",
        "    state = self._py_observe()\n",
        "    self._obs, reward, done, _ = self._env.step(action)\n",
        "    self._current_episode_total += (\n",
        "        reward * self._reward_decay ** self._current_timestep)\n",
        "    self._current_timestep += 1\n",
        "    reward = np.asarray(reward, dtype=np.float32)\n",
        "    next_state = self._py_observe()\n",
        "    ret = (reward, next_state)\n",
        "    if done:\n",
        "      self._total_rewards += self._current_episode_total\n",
        "      self._num_episodes += 1\n",
        "      self._current_episode_total = 0\n",
        "      self._obs = self._env.reset()\n",
        "    return ret\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    action = tf.squeeze(action, axis=0)  # Batch dim must be 1.\n",
        "    reward, state = tf.py_func(self._py_step,\n",
        "                               [action], [self._state_dtype, tf.float32],\n",
        "                               name=\"step\")\n",
        "    reward.set_shape(())\n",
        "    state.set_shape(self._state_shape)\n",
        "    return tf.expand_dims(reward, axis=0), tf.expand_dims(state, axis=0)\n",
        "  \n",
        "  \n",
        "  def _py_record_transition_and_sample_minibatch(\n",
        "      self, state, action, reward, next_state, batch_size):\n",
        "    for transition in zip(state, action, reward, next_state):\n",
        "      self._replay.record_transition(transition)\n",
        "    state = np.zeros((batch_size,) + state.shape[1:], dtype=state.dtype)\n",
        "    action = np.zeros((batch_size,) + action.shape[1:], dtype=action.dtype)\n",
        "    reward = np.zeros((batch_size,), dtype=reward.dtype)\n",
        "    next_state = np.zeros((batch_size,) + next_state.shape[1:],\n",
        "                          dtype=next_state.dtype)\n",
        "    for i, transition in enumerate(self._replay.sample_batch(batch_size)):\n",
        "      state[i], action[i], reward[i], next_state[i] = transition\n",
        "    return (state, action, reward, next_state)\n",
        "  \n",
        "  \n",
        "  def record_transition_and_sample_minibatch(\n",
        "      self, state, action, reward, next_state, batch_size):\n",
        "    state_shape, action_shape, next_state_shape = (\n",
        "        state.shape, action.shape, next_state.shape)\n",
        "    state, action, reward, next_state = tf.py_func(\n",
        "        self._py_record_transition_and_sample_minibatch,\n",
        "        [state, action, reward, next_state, batch_size],\n",
        "        [state.dtype, action.dtype, reward.dtype, next_state.dtype])\n",
        "    state.set_shape([None] + state_shape[1:].as_list())\n",
        "    action.set_shape([None] + action_shape[1:].as_list())\n",
        "    reward.set_shape([None])\n",
        "    next_state.set_shape([None] + next_state_shape[1:].as_list())\n",
        "    return state, action, reward, next_state\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-U9ITJ5WGOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Network Definitions\n",
        "\n",
        "# Hyper-parameters, from https://arxiv.org/abs/1509.02971.\n",
        "policy_hidden_layers = (400, 300)\n",
        "critic_hidden_layers = (400, 300)\n",
        "use_replay_buffer = True\n",
        "batch_size = 64  # Only used with replay buffers, otherwise batch_size = 1\n",
        "target_weight_decay = (1. - 0.001)\n",
        "policy_learning_rate = 1e-4\n",
        "critic_learning_rate = 1e-2\n",
        "policy_l2_regularization = 0.\n",
        "critic_l2_regularization = 1e-2\n",
        "exploration_stddev = 0.01  # Unlike the paper, we use simple Gaussian noise.\n",
        "\n",
        "\n",
        "def make_mlp(input_, layer_sizes, l2_reg=0., name=None):\n",
        "  with tf.name_scope(name, \"MLP\", [input_]):\n",
        "    net = input_\n",
        "    kernel_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
        "    for i, layer_size in enumerate(layer_sizes):\n",
        "      if i + 1 == len(layer_sizes):\n",
        "        name = \"final\"\n",
        "        activation_fn=None\n",
        "      else:\n",
        "        name = \"hidden_{}\".format(i + 1)\n",
        "        activation_fn=tf.nn.relu\n",
        "      with tf.name_scope(name):\n",
        "        net = tf.layers.dense(net, layer_size, \n",
        "                              kernel_regularizer=kernel_regularizer,\n",
        "                              activation=activation_fn)\n",
        "    return net\n",
        "\n",
        "\n",
        "def make_critic(state, action, name=None):\n",
        "  with tf.name_scope(name, \"Critic\", [state, action]):\n",
        "    with tf.name_scope(\"input\"):\n",
        "      input_ = tf.concat([state, action], axis=-1)\n",
        "    return tf.squeeze(make_mlp(input_, critic_hidden_layers + (1,),\n",
        "                               l2_reg=critic_l2_regularization),\n",
        "                      axis=-1, name=\"value_estimate\")\n",
        "  \n",
        "\n",
        "def make_policy(state, action_space, name=None):\n",
        "  with tf.name_scope(name, \"Policy\", [state]):\n",
        "    unscaled = make_mlp(state, policy_hidden_layers + action_space.shape,\n",
        "                        l2_reg=policy_l2_regularization, name=\"unscaled\")\n",
        "    high = tf.constant(action_space.high)\n",
        "    low = tf.constant(action_space.low)\n",
        "    return (0.5 + 0.5 * tf.tanh(unscaled)) * (high - low) + low\n",
        "            \n",
        "\n",
        "def record_and_sample(env, policy_network):\n",
        "  state = env.observe()\n",
        "  action = policy_network(state)\n",
        "  reward, next_state = env.step(action)\n",
        "  if not use_replay_buffer:\n",
        "    return state, action, reward, next_state\n",
        "  return env.record_transition_and_sample_minibatch(\n",
        "      state, action, reward, next_state, batch_size)\n",
        "\n",
        "\n",
        "def build_graph(env, training=True):\n",
        "  policy_network = tf.make_template(\"Policy\", make_policy,\n",
        "                                    action_space=env.action_space)\n",
        "  critic_network = tf.make_template(\"Critic\", make_critic)\n",
        "  ret = dict(policy_network=policy_network,\n",
        "             critic_network=critic_network)\n",
        "  \n",
        "  if not training:\n",
        "    return ret\n",
        "  \n",
        "  def _explore_policy(state):\n",
        "    action = policy_network(state)\n",
        "    return action + tf.random_normal(shape=tf.shape(action),\n",
        "                                     stddev=exploration_stddev)\n",
        "  \n",
        "  state, action, reward, next_state = record_and_sample(env, _explore_policy)\n",
        "  \n",
        "  policy_loss = -tf.reduce_mean(\n",
        "      critic_network(state, policy_network(state)), axis=0)\n",
        "  policy_loss += tf.losses.get_regularization_loss(\"Policy\")\n",
        "  \n",
        "  target_policy_network = tf.make_template(\"TargetPolicy\", make_policy,\n",
        "                                           action_space=env.action_space)\n",
        "  target_critic_network = tf.make_template(\"TargetCritic\", make_critic)\n",
        "  next_target_action = target_policy_network(next_state)\n",
        "  next_target_value = target_critic_network(next_state, next_target_action)\n",
        "  next_target = reward + env.reward_decay * next_target_value\n",
        "\n",
        "  critic_loss = tf.reduce_mean(\n",
        "      tf.squared_difference(critic_network(state, action), next_target), axis=0)\n",
        "  critic_loss += tf.losses.get_regularization_loss(\"Critic\")\n",
        "  \n",
        "  get_vars = lambda s: tf.contrib.framework.get_trainable_variables(scope=s)\n",
        "  policy_vars = get_vars(\"Policy\")\n",
        "  critic_vars = get_vars(\"Critic\")\n",
        "  target_policy_vars = get_vars(\"TargetPolicy\")\n",
        "  target_critic_vars = get_vars(\"TargetCritic\")\n",
        "  assert all([policy_vars, critic_vars, target_policy_vars, target_critic_vars])\n",
        "  \n",
        "  update_target_ops = []\n",
        "  init_target_ops = []\n",
        "  for target_var, orig_var in zip(target_policy_vars + target_critic_vars,\n",
        "                                  policy_vars + critic_vars):\n",
        "    update_target_ops.append(\n",
        "        tf.assign(target_var, \n",
        "                  target_var * target_weight_decay + \n",
        "                  orig_var * (1. - target_weight_decay)))\n",
        "    init_target_ops.append(tf.assign(target_var, orig_var))   \n",
        "  init_target_networks = tf.group(*init_target_ops) \n",
        "  train_policy = (tf.train.AdamOptimizer(policy_learning_rate)\n",
        "                  .minimize(policy_loss, \n",
        "                            global_step=tf.train.get_or_create_global_step(),\n",
        "                            var_list=policy_vars))\n",
        "  train_critic = (tf.train.AdamOptimizer(critic_learning_rate)\n",
        "                  .minimize(critic_loss, var_list=critic_vars))\n",
        "  with tf.control_dependencies([train_policy, train_critic]):\n",
        "    train_op = tf.group(*update_target_ops)  \n",
        "\n",
        "  ret.update(dict(\n",
        "      policy_loss=policy_loss,\n",
        "      critic_loss=critic_loss,\n",
        "      init_target_networks=init_target_networks,\n",
        "      train_op=train_op,\n",
        "  ))\n",
        "  \n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tcjVSedRkSGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/ddpg_pendulum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rI2h0S39U5kY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "586fbb85-253c-4f19-ac05-e20586068082"
      },
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "\n",
        "checkpoint_dir = \"/tmp/ddpg_pendulum\"\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  env = Env('Pendulum-v0')\n",
        "  init_vars_op = tf.global_variables_initializer()\n",
        "  endpoints = build_graph(env)\n",
        "  with tf.train.MonitoredTrainingSession(\n",
        "      checkpoint_dir=checkpoint_dir, log_step_count_steps=10000) as sess:\n",
        "    sess.run(init_vars_op)\n",
        "    sess.run(endpoints[\"init_target_networks\"])\n",
        "    for step in range(100000):\n",
        "      critic_loss_val, policy_loss_val, _ = sess.run(\n",
        "          [endpoints[\"critic_loss\"],\n",
        "           endpoints[\"policy_loss\"],\n",
        "           endpoints[\"train_op\"]])\n",
        "      if step % 10000 == 0:\n",
        "        print(\"critic loss: {}, policy loss: {}\".format(\n",
        "            critic_loss_val, policy_loss_val))\n",
        "        print(\"mean episodic reward: {}\".format(env.mean_episodic_reward()))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/ddpg_pendulum/model.ckpt.\n",
            "critic loss: 2.613407611846924, policy loss: 0.24113045632839203\n",
            "mean episodic reward: -0.9461334090949051\n",
            "INFO:tensorflow:global_step/sec: 146.291\n",
            "critic loss: 0.646467924118042, policy loss: 28.840438842773438\n",
            "mean episodic reward: -0.2378833737514516\n",
            "INFO:tensorflow:global_step/sec: 146.166\n",
            "critic loss: 0.3349519371986389, policy loss: 24.854310989379883\n",
            "mean episodic reward: -0.1201193273398419\n",
            "INFO:tensorflow:global_step/sec: 147.01\n",
            "critic loss: 0.39764660596847534, policy loss: 26.49053955078125\n",
            "mean episodic reward: -0.0803447156379075\n",
            "INFO:tensorflow:global_step/sec: 143.949\n",
            "critic loss: 0.49470388889312744, policy loss: 23.903169631958008\n",
            "mean episodic reward: -0.060358467966786226\n",
            "INFO:tensorflow:global_step/sec: 144.392\n",
            "critic loss: 0.42851388454437256, policy loss: 28.196712493896484\n",
            "mean episodic reward: -0.04833486877021527\n",
            "INFO:tensorflow:global_step/sec: 145.124\n",
            "critic loss: 0.4359409213066101, policy loss: 16.25550079345703\n",
            "mean episodic reward: -0.04030582080174097\n",
            "INFO:tensorflow:global_step/sec: 145.998\n",
            "critic loss: 0.3039846122264862, policy loss: 18.378524780273438\n",
            "mean episodic reward: -0.034564250886963056\n",
            "INFO:tensorflow:global_step/sec: 145.114\n",
            "critic loss: 0.3569753170013428, policy loss: 22.342914581298828\n",
            "mean episodic reward: -0.030254493918513797\n",
            "INFO:tensorflow:Saving checkpoints for 87306 into /tmp/ddpg_pendulum/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 145.391\n",
            "critic loss: 0.31875497102737427, policy loss: 22.505172729492188\n",
            "mean episodic reward: -0.026900337164798296\n",
            "INFO:tensorflow:global_step/sec: 145.631\n",
            "INFO:tensorflow:Saving checkpoints for 100000 into /tmp/ddpg_pendulum/model.ckpt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXxP8Sbdi0xX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ad0dc102-d01c-4a73-b1a4-c4248731aed9"
      },
      "cell_type": "code",
      "source": [
        "#@title Eval\n",
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  endpoints = build_graph(env, training=False)\n",
        "  policy_network = endpoints['policy_network']\n",
        "  critic_network = endpoints['critic_network']\n",
        "  \n",
        "  state = env.reset()\n",
        "  state_ph = tf.placeholder(tf.float32, shape=(None,) + state.shape)\n",
        "  \n",
        "  action = policy_network(state_ph)\n",
        "  value = critic_network(state_ph, action)\n",
        "\n",
        "  with tf.train.MonitoredSession(\n",
        "      session_creator=tf.train.ChiefSessionCreator(\n",
        "          checkpoint_dir=checkpoint_dir)) as sess:\n",
        "    done = False\n",
        "    undiscounted_reward = 0\n",
        "    num_steps = 0\n",
        "    while not done:\n",
        "      action_val, value_val = sess.run(\n",
        "          [action, value], feed_dict={\n",
        "              state_ph: np.asarray(state[np.newaxis], dtype=np.float32)})\n",
        "      state, reward, done, _ = env.step(action_val[0])\n",
        "      undiscounted_reward += reward\n",
        "      num_steps += 1\n",
        "      # print(\"State:\", state, \"Action\", action_val, \"Value:\", value_val)\n",
        "  \n",
        "  print(\"Done. Mean reward: {} ({} steps)\".format(\n",
        "      undiscounted_reward / num_steps, num_steps))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/ddpg_pendulum/model.ckpt-100000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Done. Mean reward: -2.649027388043568 (200 steps)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}